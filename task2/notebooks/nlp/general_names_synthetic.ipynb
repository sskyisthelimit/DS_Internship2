{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382f4257",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-20T11:37:47.317521Z",
     "iopub.status.busy": "2025-02-20T11:37:47.317099Z",
     "iopub.status.idle": "2025-02-20T11:37:59.301074Z",
     "shell.execute_reply": "2025-02-20T11:37:59.300231Z"
    },
    "papermill": {
     "duration": 11.99084,
     "end_time": "2025-02-20T11:37:59.303288",
     "exception": false,
     "start_time": "2025-02-20T11:37:47.312448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (2.10.1)\r\n",
      "Collecting bitsandbytes-cuda110\r\n",
      "  Downloading bitsandbytes_cuda110-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from pydantic) (4.12.2)\r\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\r\n",
      "Downloading bitsandbytes_cuda110-0.26.0.post2-py3-none-any.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes-cuda110, bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.45.2 bitsandbytes-cuda110-0.26.0.post2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pydantic bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d7f276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T11:37:59.312196Z",
     "iopub.status.busy": "2025-02-20T11:37:59.311902Z",
     "iopub.status.idle": "2025-02-20T11:37:59.322922Z",
     "shell.execute_reply": "2025-02-20T11:37:59.322099Z"
    },
    "papermill": {
     "duration": 0.01721,
     "end_time": "2025-02-20T11:37:59.324442",
     "exception": false,
     "start_time": "2025-02-20T11:37:59.307232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'lion', 'fish', 'spider', 'fly', 'butterfly', 'horse', 'bull', 'cow', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/general-animal-names/general_animal_names.json\") as json_data:\n",
    "    animal_samples = json.load(json_data)[\"animals\"]\n",
    "\n",
    "print(animal_samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffbe275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T11:37:59.332949Z",
     "iopub.status.busy": "2025-02-20T11:37:59.332202Z",
     "iopub.status.idle": "2025-02-20T11:39:46.060194Z",
     "shell.execute_reply": "2025-02-20T11:39:46.059187Z"
    },
    "papermill": {
     "duration": 106.734391,
     "end_time": "2025-02-20T11:39:46.062348",
     "exception": false,
     "start_time": "2025-02-20T11:37:59.327957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741a7240af446fe831901051a285ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Span\n",
    "import re\n",
    "import gc \n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "model_dir = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# # Load CSV files\n",
    "# animal_df = pd.read_csv(\"/kaggle/input/animal-names/processed_animal_names.csv\")\n",
    "# needed_animal_classes = [\"Mammalia\", \"Aves\", \"Arachnida\", \"Insecta\", \"Pisces\", \"Amphibia\", \"Mollusca\", \"Crustacea\", \"Cnidaria\"]\n",
    "\n",
    "# animal_samples = list(animal_df[animal_df[\"class\"].isin(needed_animal_classes)][\"common_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab99f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T11:39:46.072539Z",
     "iopub.status.busy": "2025-02-20T11:39:46.071940Z",
     "iopub.status.idle": "2025-02-20T11:39:46.077696Z",
     "shell.execute_reply": "2025-02-20T11:39:46.076846Z"
    },
    "papermill": {
     "duration": 0.013216,
     "end_time": "2025-02-20T11:39:46.079436",
     "exception": false,
     "start_time": "2025-02-20T11:39:46.066220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an advanced AI trained in natural language processing and synthetic data generation.\n",
    "Your task is to read the following animal name and generate 10 unique sentences using given animal name.\n",
    "Make main focus on diversifying sentences - sentence structures and words.\n",
    "\n",
    "Make sure to extract the exact and use string of the animal name without any changes in it.\n",
    "For each sentence, highlight the name of the given animal string by setting \"||\" around it.\n",
    "You are not allowed to use words that may have a meaning of the animal except given animal name.\n",
    "Do not provide any explanations.\n",
    "Only respond with the JSON structured data, structure of JSON should be strictly as in examples.\n",
    "\n",
    "### Example 1:\n",
    "Input: 'bald eagle'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "        \"bald eagle\": [\n",
    "            \"The majestic ||bald eagle|| soars high above the tranquil lake, its keen eyes scanning for prey.\",\n",
    "            \"With powerful wings, the ||bald eagle|| glides effortlessly through the morning sky.\",\n",
    "            \"A symbol of strength and freedom, the ||bald eagle|| commands attention wherever it flies.\",\n",
    "            \"The sharp talons of the ||bald eagle|| make it a formidable hunter among the skies.\",\n",
    "            \"The call of the ||bald eagle|| echoes through the valleys, a sound both haunting and beautiful.\",\n",
    "            \"Under the golden sunset, the silhouette of the ||bald eagle|| is a breathtaking sight.\",\n",
    "            \"The ||bald eagle|| is often seen perched on rocky cliffs, surveying the world below.\",\n",
    "            \"The ||bald eagle|| is easily identifiable by its white head and tail feathers.\",\n",
    "            \"From a distance, it was hard to tell that was the ||bald eagle||.\",\n",
    "            \"||eagle|| enthusiasts pay much attention to ||bald eagle||, admiring its regal presence and hunting prowess.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "### Example 2:\n",
    "Input: 'cow'\n",
    "\n",
    "Output:\n",
    "[\n",
    "    {\n",
    "        \"cow\": [\n",
    "            \"In the serene meadow, the ||cow|| grazes peacefully under the warm sun.\",\n",
    "            \"Once revered in ancient cultures, the ||cow|| holds symbolic meaning even today.\",\n",
    "            \"Researchers study the digestion of the ||cow|| to improve agricultural efficiency.\",\n",
    "            \"The ||cow||, known for its gentle demeanor, is a beloved farm animal worldwide.\",\n",
    "            \"On the rolling hills, the ||cow|| is a symbol of pastoral beauty.\",\n",
    "            \"||Cow|| enthusiasts pay much attention for ||cows|| with exceptional milk production.\",\n",
    "            \"Farmers appreciate the ||cow|| not only for milk but also for the role of the ||cow|| in sustainable agriculture.\",\n",
    "            \"The bell around the ||cow||'s neck jingles as the ||cow|| moves through the pasture.\",\n",
    "            \"From ancient myths to modern-day farming, the ||cow|| has always held a special place in human society, symbolizing abundance and nurturing.\",\n",
    "            \"The ||cow||'s milk is used to make cheese, yogurt, and butter, highlighting the ||cow||'s importance in the culinary world.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "Continue with the task and stop after generating valid output for the given animal by the user by outputting '### Output ends here.'\n",
    "Don't forget this strict rules.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d83851c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T11:39:46.087672Z",
     "iopub.status.busy": "2025-02-20T11:39:46.087221Z",
     "iopub.status.idle": "2025-02-20T13:03:38.061595Z",
     "shell.execute_reply": "2025-02-20T13:03:38.060702Z"
    },
    "papermill": {
     "duration": 5031.980916,
     "end_time": "2025-02-20T13:03:38.063893",
     "exception": false,
     "start_time": "2025-02-20T11:39:46.082977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/239 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   0%|          | 1/239 [00:23<1:32:49, 23.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   1%|          | 2/239 [00:45<1:29:36, 22.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   1%|▏         | 3/239 [01:02<1:19:04, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   2%|▏         | 4/239 [01:24<1:22:07, 20.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   2%|▏         | 5/239 [01:47<1:23:33, 21.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   3%|▎         | 6/239 [02:09<1:24:24, 21.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   3%|▎         | 7/239 [02:20<1:11:06, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   3%|▎         | 8/239 [02:35<1:05:53, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   4%|▍         | 9/239 [02:57<1:11:44, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   4%|▍         | 10/239 [03:19<1:15:37, 19.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   5%|▍         | 11/239 [03:42<1:18:13, 20.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   5%|▌         | 12/239 [04:05<1:20:28, 21.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   5%|▌         | 13/239 [04:27<1:21:41, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   6%|▌         | 14/239 [04:50<1:22:13, 21.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   6%|▋         | 15/239 [05:05<1:14:43, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   7%|▋         | 16/239 [05:28<1:17:04, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   7%|▋         | 17/239 [05:50<1:18:40, 21.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   8%|▊         | 18/239 [06:06<1:12:42, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   8%|▊         | 19/239 [06:21<1:07:17, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   8%|▊         | 20/239 [06:44<1:11:17, 19.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   9%|▉         | 21/239 [07:06<1:13:57, 20.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:   9%|▉         | 22/239 [07:28<1:15:45, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  10%|▉         | 23/239 [07:51<1:16:46, 21.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  10%|█         | 24/239 [08:13<1:17:27, 21.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  10%|█         | 25/239 [08:35<1:17:45, 21.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  11%|█         | 26/239 [08:57<1:17:52, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  11%|█▏        | 27/239 [09:20<1:17:53, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  12%|█▏        | 28/239 [09:42<1:17:45, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  12%|█▏        | 29/239 [10:04<1:17:38, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  13%|█▎        | 30/239 [10:27<1:17:21, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  13%|█▎        | 31/239 [10:41<1:09:15, 19.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  13%|█▎        | 32/239 [11:04<1:11:16, 20.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  14%|█▍        | 33/239 [11:26<1:12:44, 21.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  14%|█▍        | 34/239 [11:48<1:13:21, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  15%|█▍        | 35/239 [12:10<1:13:52, 21.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  15%|█▌        | 36/239 [12:33<1:14:04, 21.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  15%|█▌        | 37/239 [12:55<1:14:02, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  16%|█▌        | 38/239 [13:17<1:13:56, 22.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  16%|█▋        | 39/239 [13:40<1:13:50, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  17%|█▋        | 40/239 [13:57<1:08:54, 20.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  17%|█▋        | 41/239 [14:19<1:10:04, 21.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  18%|█▊        | 42/239 [14:42<1:10:42, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  18%|█▊        | 43/239 [15:04<1:11:02, 21.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  18%|█▊        | 44/239 [15:26<1:11:08, 21.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  19%|█▉        | 45/239 [15:41<1:03:50, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  19%|█▉        | 46/239 [15:55<58:33, 18.21s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  20%|█▉        | 47/239 [16:11<56:01, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  20%|██        | 48/239 [16:34<1:00:20, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  21%|██        | 49/239 [16:56<1:03:13, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  21%|██        | 50/239 [17:18<1:05:00, 20.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  21%|██▏       | 51/239 [17:40<1:06:06, 21.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  22%|██▏       | 52/239 [18:03<1:06:44, 21.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  22%|██▏       | 53/239 [18:25<1:07:05, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  23%|██▎       | 54/239 [18:47<1:07:21, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  23%|██▎       | 55/239 [19:09<1:07:18, 21.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  23%|██▎       | 56/239 [19:24<1:00:47, 19.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  24%|██▍       | 57/239 [19:40<56:35, 18.66s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  24%|██▍       | 58/239 [20:02<59:27, 19.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  25%|██▍       | 59/239 [20:24<1:01:21, 20.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  25%|██▌       | 60/239 [20:39<55:57, 18.76s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  26%|██▌       | 61/239 [21:02<58:57, 19.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  26%|██▌       | 62/239 [21:17<54:05, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  26%|██▋       | 63/239 [21:39<57:23, 19.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Extra data: line 5 column 1 (char 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 64/239 [21:55<53:53, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  27%|██▋       | 65/239 [22:17<56:54, 19.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  28%|██▊       | 66/239 [22:40<58:55, 20.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Expecting ',' delimiter: line 14 column 10 (char 1163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 67/239 [23:02<1:00:15, 21.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  28%|██▊       | 68/239 [23:24<1:00:58, 21.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  29%|██▉       | 69/239 [23:46<1:01:23, 21.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  29%|██▉       | 70/239 [24:09<1:01:34, 21.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  30%|██▉       | 71/239 [24:31<1:01:30, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  30%|███       | 72/239 [24:45<54:23, 19.54s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  31%|███       | 73/239 [25:07<56:23, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  31%|███       | 74/239 [25:29<57:27, 20.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  31%|███▏      | 75/239 [25:52<58:11, 21.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  32%|███▏      | 76/239 [26:14<58:41, 21.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  32%|███▏      | 77/239 [26:29<52:41, 19.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  33%|███▎      | 78/239 [26:51<54:37, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON start character found in segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 79/239 [27:13<55:47, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  33%|███▎      | 80/239 [27:35<56:28, 21.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  34%|███▍      | 81/239 [27:58<56:54, 21.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  34%|███▍      | 82/239 [28:20<57:10, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  35%|███▍      | 83/239 [28:42<57:06, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  35%|███▌      | 84/239 [29:04<56:56, 22.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  36%|███▌      | 85/239 [29:19<50:37, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  36%|███▌      | 86/239 [29:41<52:10, 20.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  36%|███▋      | 87/239 [29:57<48:27, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  37%|███▋      | 88/239 [30:19<50:23, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  37%|███▋      | 89/239 [30:41<51:39, 20.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  38%|███▊      | 90/239 [31:04<52:29, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  38%|███▊      | 91/239 [31:26<52:57, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  38%|███▊      | 92/239 [31:40<47:23, 19.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  39%|███▉      | 93/239 [31:54<43:09, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  39%|███▉      | 94/239 [32:09<40:57, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  40%|███▉      | 95/239 [32:31<44:27, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  40%|████      | 96/239 [32:54<46:46, 19.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON start character found in segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 97/239 [33:16<48:16, 20.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  41%|████      | 98/239 [33:38<49:13, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  41%|████▏     | 99/239 [34:00<49:45, 21.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  42%|████▏     | 100/239 [34:18<47:08, 20.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  42%|████▏     | 101/239 [34:41<48:08, 20.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON start character found in segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|████▎     | 102/239 [34:56<44:06, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  43%|████▎     | 103/239 [35:18<45:49, 20.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  44%|████▎     | 104/239 [35:41<46:53, 20.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  44%|████▍     | 105/239 [36:03<47:25, 21.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  44%|████▍     | 106/239 [36:25<47:44, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  45%|████▍     | 107/239 [36:47<47:52, 21.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  45%|████▌     | 108/239 [37:10<47:49, 21.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  46%|████▌     | 109/239 [37:32<47:40, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  46%|████▌     | 110/239 [37:54<47:24, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  46%|████▋     | 111/239 [38:16<47:10, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  47%|████▋     | 112/239 [38:39<46:50, 22.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  47%|████▋     | 113/239 [39:01<46:33, 22.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Invalid control character at: line 10 column 133 (char 885)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 114/239 [39:23<46:08, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  48%|████▊     | 115/239 [39:45<45:45, 22.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  49%|████▊     | 116/239 [40:07<45:25, 22.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  49%|████▉     | 117/239 [40:29<45:02, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  49%|████▉     | 118/239 [40:52<44:41, 22.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  50%|████▉     | 119/239 [41:14<44:22, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  50%|█████     | 120/239 [41:36<44:02, 22.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  51%|█████     | 121/239 [41:58<43:41, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  51%|█████     | 122/239 [42:20<43:18, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  51%|█████▏    | 123/239 [42:43<42:56, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 124/239 [43:05<42:33, 22.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 125/239 [43:27<42:10, 22.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 126/239 [43:43<38:22, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 127/239 [44:05<39:02, 20.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  54%|█████▎    | 128/239 [44:27<39:21, 21.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  54%|█████▍    | 129/239 [44:50<39:31, 21.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  54%|█████▍    | 130/239 [45:12<39:30, 21.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON start character found in segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|█████▍    | 131/239 [45:29<36:42, 20.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  55%|█████▌    | 132/239 [45:51<37:19, 20.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  56%|█████▌    | 133/239 [46:14<37:42, 21.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  56%|█████▌    | 134/239 [46:36<37:47, 21.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Extra data: line 5 column 1 (char 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|█████▋    | 135/239 [46:58<37:45, 21.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 136/239 [47:20<37:36, 21.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 137/239 [47:43<37:29, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 138/239 [48:05<37:14, 22.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 139/239 [48:27<36:56, 22.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  59%|█████▊    | 140/239 [48:49<36:34, 22.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 141/239 [49:11<36:10, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 142/239 [49:34<35:47, 22.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  60%|█████▉    | 143/239 [49:56<35:22, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  60%|██████    | 144/239 [50:18<34:57, 22.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  61%|██████    | 145/239 [50:40<34:36, 22.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  61%|██████    | 146/239 [51:02<34:14, 22.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 147/239 [51:24<33:49, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 148/239 [51:46<33:26, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 149/239 [52:08<33:03, 22.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 150/239 [52:28<31:55, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 151/239 [52:50<31:49, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  64%|██████▎   | 152/239 [53:12<31:35, 21.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  64%|██████▍   | 153/239 [53:34<31:21, 21.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  64%|██████▍   | 154/239 [53:56<31:00, 21.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  65%|██████▍   | 155/239 [54:18<30:39, 21.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  65%|██████▌   | 156/239 [54:40<30:21, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  66%|██████▌   | 157/239 [55:02<30:00, 21.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  66%|██████▌   | 158/239 [55:24<29:41, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 159/239 [55:46<29:18, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 160/239 [56:08<28:56, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 161/239 [56:30<28:35, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 162/239 [56:52<28:13, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 163/239 [57:14<27:49, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  69%|██████▊   | 164/239 [57:36<27:25, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 165/239 [57:58<27:08, 22.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 166/239 [58:20<26:46, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  70%|██████▉   | 167/239 [58:42<26:23, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  70%|███████   | 168/239 [59:04<26:00, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  71%|███████   | 169/239 [59:26<25:39, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  71%|███████   | 170/239 [59:48<25:16, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 171/239 [1:00:07<23:57, 21.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 172/239 [1:00:20<20:57, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 173/239 [1:00:43<21:42, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 174/239 [1:01:05<22:08, 20.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 175/239 [1:01:20<20:04, 18.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  74%|███████▎  | 176/239 [1:01:42<20:48, 19.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  74%|███████▍  | 177/239 [1:02:04<21:08, 20.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  74%|███████▍  | 178/239 [1:02:26<21:15, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  75%|███████▍  | 179/239 [1:02:41<19:07, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 180/239 [1:03:03<19:41, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  76%|███████▌  | 181/239 [1:03:25<20:05, 20.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  76%|███████▌  | 182/239 [1:03:48<20:12, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 183/239 [1:04:10<20:10, 21.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 184/239 [1:04:32<19:59, 21.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 185/239 [1:04:55<19:44, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 186/239 [1:05:17<19:30, 22.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 187/239 [1:05:34<17:45, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  79%|███████▊  | 188/239 [1:05:56<17:50, 20.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 189/239 [1:06:18<17:45, 21.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 190/239 [1:06:40<17:33, 21.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  80%|███████▉  | 191/239 [1:07:02<17:20, 21.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  80%|████████  | 192/239 [1:07:24<17:05, 21.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  81%|████████  | 193/239 [1:07:46<16:46, 21.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  81%|████████  | 194/239 [1:08:08<16:26, 21.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 195/239 [1:08:30<16:07, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 196/239 [1:08:52<15:45, 22.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 197/239 [1:09:14<15:23, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 198/239 [1:09:36<14:51, 21.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 199/239 [1:09:49<12:52, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  84%|████████▎ | 200/239 [1:10:11<13:05, 20.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  84%|████████▍ | 201/239 [1:10:33<13:06, 20.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  85%|████████▍ | 202/239 [1:10:55<12:59, 21.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Extra data: line 4 column 1 (char 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 203/239 [1:11:17<12:49, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  85%|████████▌ | 204/239 [1:11:39<12:36, 21.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  86%|████████▌ | 205/239 [1:12:01<12:18, 21.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  86%|████████▌ | 206/239 [1:12:23<11:58, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 207/239 [1:12:44<11:26, 21.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 208/239 [1:12:59<10:04, 19.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 209/239 [1:13:21<10:08, 20.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 210/239 [1:13:43<10:03, 20.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 211/239 [1:14:05<09:52, 21.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  89%|████████▊ | 212/239 [1:14:27<09:38, 21.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  89%|████████▉ | 213/239 [1:14:49<09:22, 21.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  90%|████████▉ | 214/239 [1:15:11<09:03, 21.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  90%|████████▉ | 215/239 [1:15:33<08:42, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  90%|█████████ | 216/239 [1:15:55<08:22, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  91%|█████████ | 217/239 [1:16:17<08:01, 21.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  91%|█████████ | 218/239 [1:16:32<06:56, 19.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 219/239 [1:16:54<06:49, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 220/239 [1:17:16<06:37, 20.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 221/239 [1:17:38<06:22, 21.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 222/239 [1:18:00<06:05, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 223/239 [1:18:22<05:45, 21.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  94%|█████████▎| 224/239 [1:18:44<05:26, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  94%|█████████▍| 225/239 [1:19:00<04:38, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 226/239 [1:19:22<04:26, 20.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 227/239 [1:19:44<04:11, 20.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  95%|█████████▌| 228/239 [1:20:01<03:39, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  96%|█████████▌| 229/239 [1:20:23<03:25, 20.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  96%|█████████▌| 230/239 [1:20:45<03:08, 20.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 231/239 [1:21:01<02:34, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 232/239 [1:21:23<02:20, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 233/239 [1:21:45<02:04, 20.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 234/239 [1:22:07<01:45, 21.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 235/239 [1:22:29<01:25, 21.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  99%|█████████▊| 236/239 [1:22:51<01:04, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches:  99%|█████████▉| 237/239 [1:23:13<00:43, 21.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches: 100%|█████████▉| 238/239 [1:23:35<00:21, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing batches: 100%|██████████| 239/239 [1:23:51<00:00, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON: Expecting ',' delimiter: line 9 column 95 (char 672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "import os\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "failed_animal_names = []\n",
    "import json\n",
    "import re\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"\n",
    "    Extracts JSON content from the response string.\n",
    "    It looks for the first occurrence of '[' or '{' after the keyword \"Output:\" \n",
    "    and uses the last occurrence of the corresponding closing bracket before \"### Output ends here.\".\n",
    "    If extraction or parsing fails, returns an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Locate the start of the output segment and the end marker\n",
    "        output_index = response.find(\"\\nUser:\")\n",
    "        end_index = response.rfind(\"Output ends here\")\n",
    "        if output_index == -1 or end_index == -1:\n",
    "            print(\"Output or end marker not found\")\n",
    "            return []\n",
    "        # Consider only the text between \"Output:\" and \"### Output ends here.\"\n",
    "        segment = response[output_index:end_index]\n",
    "        # Find the first occurrence of '[' or '{' in the segment\n",
    "        match = re.search(r'([\\[\\{])', segment)\n",
    "        if not match:\n",
    "            print(\"No JSON start character found in segment\")\n",
    "            return []\n",
    "        start_bracket = match.start()\n",
    "        json_content = segment[start_bracket:].strip()\n",
    "        # Depending on the first character, find the last matching closing bracket\n",
    "        if json_content[0] == '[':\n",
    "            end_bracket = json_content.rfind(']')\n",
    "        else:\n",
    "            end_bracket = json_content.rfind('}')\n",
    "        if end_bracket == -1:\n",
    "            print(\"No closing bracket found\")\n",
    "            return []\n",
    "        json_content = json_content[:end_bracket+1]\n",
    "        # Debug print the extracted JSON content\n",
    "        parsed = json.loads(json_content)\n",
    "        # Wrap dictionary in a list if needed\n",
    "        if not isinstance(parsed, list):\n",
    "            parsed = [parsed]\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_stopping_criteria(stop_words, tokenizer, device):\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], device=device, encounters = 1):\n",
    "            super().__init__()\n",
    "            self.stops = stops = [stop.to(device) for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: LongTensor, scores: FloatTensor) -> bool:\n",
    "            last_token = input_ids[0][-1]\n",
    "            for stop in self.stops:\n",
    "                if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    stop_word_ids = [tokenizer(stop_word,\n",
    "                               return_tensors=\"pt\", \n",
    "                               add_special_tokens=False)[\"input_ids\"].squeeze() \n",
    "                               for stop_word in stop_words]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_word_ids)])\n",
    "    return stopping_criteria\n",
    "\n",
    "\n",
    "stop_words_list = [\"Output ends\"]\n",
    "stopping_criteria = None\n",
    "if stop_words_list is not None:\n",
    "    stopping_criteria = create_stopping_criteria(stop_words_list, tokenizer, device)\n",
    "\n",
    "def write_batch_to_json(entities, start_idx):\n",
    "    filename = os.path.join(\"/kaggle/working/generated/\", \"entity_\" + str(start_idx) + \".json\")\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities, f)\n",
    "        \n",
    "\n",
    "def predict_entities_in_batches(test_dataset, model, tokenizer, system_prompt):\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_dataset)), desc=\"Processing batches\"):\n",
    "\n",
    "            prompt = test_dataset[i]\n",
    "            prompt = f\"{prompt}\"\n",
    "            chat_input = [\n",
    "                f\"{system_prompt}\\nUser: {prompt}\"\n",
    "            ]\n",
    "            results = text_generation_pipeline(chat_input,\n",
    "                                               max_new_tokens=400,\n",
    "                                               do_sample=True,\n",
    "                                               temperature=1.1,\n",
    "                                               top_p=0.9)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            entities = []\n",
    "            for result in results:\n",
    "                generated_text = result[0]['generated_text']\n",
    "                entity = extract_json_from_response(generated_text)\n",
    "                if entity == []:\n",
    "                    failed_animal_names.append(prompt)\n",
    "                else:\n",
    "                    entities.append(entity)\n",
    "            \n",
    "            write_batch_to_json(entities, i) \n",
    "\n",
    "\n",
    "os.makedirs(\"/kaggle/working/generated/\", exist_ok=True)\n",
    "\n",
    "processed_data = predict_entities_in_batches(animal_samples, model, tokenizer, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51fdb281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T13:03:38.112485Z",
     "iopub.status.busy": "2025-02-20T13:03:38.112018Z",
     "iopub.status.idle": "2025-02-20T13:03:38.115981Z",
     "shell.execute_reply": "2025-02-20T13:03:38.115307Z"
    },
    "papermill": {
     "duration": 0.029503,
     "end_time": "2025-02-20T13:03:38.117403",
     "exception": false,
     "start_time": "2025-02-20T13:03:38.087900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"/kaggle/working/failed_animal_names.json\"\n",
    "with open(filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump({\"failed_animal_names\": failed_animal_names}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf652c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T13:03:38.167191Z",
     "iopub.status.busy": "2025-02-20T13:03:38.166681Z",
     "iopub.status.idle": "2025-02-20T13:03:38.204193Z",
     "shell.execute_reply": "2025-02-20T13:03:38.203362Z"
    },
    "papermill": {
     "duration": 0.065104,
     "end_time": "2025-02-20T13:03:38.205769",
     "exception": false,
     "start_time": "2025-02-20T13:03:38.140665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/generated.zip'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/generated', \"zip\", '/kaggle/working/generated/')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6700331,
     "sourceId": 10796151,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6704152,
     "sourceId": 10801545,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6705927,
     "sourceId": 10804041,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5156.641277,
   "end_time": "2025-02-20T13:03:41.554110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-20T11:37:44.912833",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "296f2844a906445191a6ced303237a8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "355438ec67c144a481d4a884e9d9d594": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7200a8941cc24ce3a52efb0cced29f52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2fe53725b72483ca0e384f4b4563be1",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_296f2844a906445191a6ced303237a8c",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "8741a7240af446fe831901051a285ad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad981db7ec3a4ecaa2308dd31652d004",
        "IPY_MODEL_7200a8941cc24ce3a52efb0cced29f52",
        "IPY_MODEL_dcbdca8e9ec5458884df57775784b801"
       ],
       "layout": "IPY_MODEL_bbe203f9c3ce4dcda6536786ca87e18b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "91ef83ac6d5b4763a634934de9c8669b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad981db7ec3a4ecaa2308dd31652d004": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_91ef83ac6d5b4763a634934de9c8669b",
       "placeholder": "​",
       "style": "IPY_MODEL_c1d691741f00471ca508a7afaab25c51",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "bbe203f9c3ce4dcda6536786ca87e18b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1d691741f00471ca508a7afaab25c51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ce7aa38c40d14641826f098a5fcdd37d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2fe53725b72483ca0e384f4b4563be1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcbdca8e9ec5458884df57775784b801": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_355438ec67c144a481d4a884e9d9d594",
       "placeholder": "​",
       "style": "IPY_MODEL_ce7aa38c40d14641826f098a5fcdd37d",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [01:24&lt;00:00, 18.11s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
